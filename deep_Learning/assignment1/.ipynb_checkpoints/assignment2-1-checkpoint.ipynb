{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a417f97",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m X, y \u001b[38;5;241m=\u001b[39m load_and_preprocess_data(filename)\n\u001b[0;32m     17\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m X\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def load_and_preprocess_data(filename):\n",
    "    data = pd.read_csv(filename, header=None)  \n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y\n",
    "filename = 'diabetes.csv'  \n",
    "X, y = load_and_preprocess_data(filename)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fbddcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.63994726,  0.84832379,  0.14964075,  0.90726993, -0.69289057,\n",
       "        0.20401277,  0.46849198,  1.4259954 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2bb9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef80605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.42622834831294626\n",
      "Epoch 100: Loss 0.05190160086613634\n",
      "Epoch 200: Loss 0.03337157701035387\n",
      "Epoch 300: Loss 0.025883798607101192\n",
      "Epoch 400: Loss 0.021668310032425222\n",
      "Epoch 500: Loss 0.01890512352862961\n",
      "Epoch 600: Loss 0.016926898969267745\n",
      "Epoch 700: Loss 0.015426495966271154\n",
      "Epoch 800: Loss 0.014241175229107303\n",
      "Epoch 900: Loss 0.013275955324798187\n",
      "Final Weights (Hidden Layer):\n",
      "[[ 1.76405235 -0.0081971   1.08824939  2.22163246]\n",
      " [ 1.86755799 -1.52006036  1.09525839 -0.17688954]\n",
      " [-0.10321885  0.33100837  0.16965094  1.44976971]\n",
      " [ 0.76103773 -0.18676052  0.47877505  0.32753407]\n",
      " [ 1.49407907 -0.39525705  0.39323871 -0.86819613]\n",
      " [-2.55298982  0.47030343  1.10846062 -0.78508376]]\n",
      "Final Biases (Hidden Layer):\n",
      "[[ 2.36887176e-12  3.58318397e-01  1.71125669e-01 -3.00973930e-02]]\n",
      "Final Weights (Output Layer):\n",
      "[[ 2.26975462]\n",
      " [-1.81592603]\n",
      " [ 1.06944936]\n",
      " [-0.17579473]]\n",
      "Final Biases (Output Layer):\n",
      "[[0.05257103]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define sigmoid and ReLU activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Initialize random weights and biases for the hidden and output layers\n",
    "np.random.seed(0)\n",
    "input_size = 6  # Assuming 6 input features\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize weights and biases with random values\n",
    "weights_hidden = np.random.randn(input_size, hidden_size)\n",
    "bias_hidden = np.zeros((1, hidden_size))\n",
    "weights_output = np.random.randn(hidden_size, output_size)\n",
    "bias_output = np.zeros((1, output_size))\n",
    "\n",
    "# Input data\n",
    "X = np.array([[ 0.63994726,  0.84832379,  0.14964075,  0.20401277,  0.46849198,  1.4259954 ],\n",
    "              [-0.84488505, -1.12339636, -0.16054575, -0.68442195, -0.36506078, -0.19067191],\n",
    "              [ 1.23388019,  1.94372388, -0.26394125, -1.10325546,  0.60439732, -0.10558415]])\n",
    "\n",
    "# Output data (target values)\n",
    "Y = np.array([[1], [0], [1]])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation\n",
    "    hidden_input = np.dot(X, weights_hidden) + bias_hidden\n",
    "    hidden_output = relu(hidden_input)\n",
    "    output_layer_input = np.dot(hidden_output, weights_output) + bias_output\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "    \n",
    "    # Calculate the loss (binary cross-entropy)\n",
    "    loss = -np.mean(Y * np.log(output_layer_output) + (1 - Y) * np.log(1 - output_layer_output))\n",
    "    \n",
    "    # Backpropagation\n",
    "    d_loss = output_layer_output - Y\n",
    "    d_output = d_loss * output_layer_output * (1 - output_layer_output)\n",
    "    d_hidden = np.dot(d_output, weights_output.T) * (hidden_output > 0)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    weights_output -= learning_rate * np.dot(hidden_output.T, d_output)\n",
    "    bias_output -= learning_rate * np.sum(d_output, axis=0, keepdims=True)\n",
    "    weights_hidden -= learning_rate * np.dot(X.T, d_hidden)\n",
    "    bias_hidden -= learning_rate * np.sum(d_hidden, axis=0, keepdims=True)\n",
    "    \n",
    "    # Print the loss for every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {loss}\")\n",
    "\n",
    "# Print the final model parameters\n",
    "print(\"Final Weights (Hidden Layer):\")\n",
    "print(weights_hidden)\n",
    "print(\"Final Biases (Hidden Layer):\")\n",
    "print(bias_hidden)\n",
    "print(\"Final Weights (Output Layer):\")\n",
    "print(weights_output)\n",
    "print(\"Final Biases (Output Layer):\")\n",
    "print(bias_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21beea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93d384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
